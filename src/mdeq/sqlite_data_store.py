"""
Sqlite3 Data stores record data as binary blobs, which enables storing
compressed data. These inherit from cogent3.app.data_store.DataStoreBase

Each DataStore records a single actionable data type. For example, all members
of the complete table represent ArrayAlignment instances. Irrespective of what
that type is, all members of the incomplete table correspond to NotCompleted
type, all members of the log table are just text output from a scitrack
log.

The complete, incomplete, log tables all have a "data" column, which is a
binary. For complete and incomplete, that may be a pickled dict with a 'data'
key whose value is blosc2 compressed. Reading from the data store returns that
dict with values wrapped in a CompressedValue instance. The latter has
decompressed and deserialised property's that handle the deserialisation steps.
"""
import contextlib
import datetime
import json
import os
import pathlib
import pickle
import re
import reprlib
import sqlite3

from collections import defaultdict
from typing import Callable, Optional, Union

from blosc2 import compress
from cogent3 import get_app
from cogent3.app import io
from cogent3.app.composable import WRITER, NotCompleted, define_app
from cogent3.app.data_store import (
    RAISE,
    DataStoreMember,
    ReadOnlyDataStoreBase,
    get_data_source,
)
from cogent3.app.typing import IdentifierType, SerialisableType
from cogent3.util.deserialise import deserialise_not_completed
from cogent3.util.table import Table
from scitrack import get_text_hexdigest

from mdeq.utils import CompressedValue


_INCOMPLETE_TABLE = "incomplete"
_COMPLETE_TABLE = "complete"
_LOG_TABLE = "logs"
_MEMORY = ":memory:"
_mem_pattern = re.compile(r"^\s*[:]{0,1}memory[:]{0,1}\s*$")


def _serialised_compressed(data: dict) -> bytes:
    """blosc compressed serialised data

    Notes
    -----
    Trys using json.dumps() on dict values, then pickle.dumps() dict.
    If json.dumps() fails, falls back to pickle.dumps().
    """
    try:
        return pickle.dumps(
            {k: compress(json.dumps(v).encode("utf8")) for k, v in data.items()}
        )
    except TypeError:
        return pickle.dumps({k: compress(pickle.dumps(v)) for k, v in data.items()})


def _deserialised_compressed(data: bytes) -> dict:
    """unpickle top-level dict, leaving dict values compressed

    Parameters
    ----------
    data : bytes
        Generated by _serialised_compressed()

    Returns
    -------
    dict values remain compressed, wrapped as CompressedValue instances
    """
    data = pickle.loads(data)
    for k, v in data.items():
        data[k] = CompressedValue(v)
    return data


# create db
def open_sqlite_db_rw(path: Union[str, pathlib.Path]):
    """creates a new sqlitedb for read/write at path, can be an in-memory db

    Notes
    -----
    This function embeds the schema. There are four tables:
    - complete: completed analysis objects
    - incomplete: NotCompleted objects
    - logs: log-file contents
    - state: whether db is locked to a process

    Returns
    -------
    Handle to a sqlite3 session
    """
    raise RuntimeError

    db = sqlite3.connect(
        path,
        isolation_level=None,
        detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
    )
    db.row_factory = sqlite3.Row
    create_template = "CREATE TABLE IF NOT EXISTS {};"
    # note it is essential to use INTEGER for the autoincrement of primary key to work
    creates = [
        "state(state_id INTEGER PRIMARY KEY, record_type TEXT, lock_pid INTEGER)",
        f"{_LOG_TABLE}(log_id INTEGER PRIMARY KEY, date timestamp, data BLOB)",
        f"{_COMPLETE_TABLE}(record_id TEXT PRIMARY KEY, log_id INTEGER, data BLOB)",
        f"{_INCOMPLETE_TABLE}(record_id TEXT PRIMARY KEY, log_id INTEGER, data BLOB)",
    ]
    for table in creates:
        db.execute(create_template.format(table))
    return db


def open_sqlite_db_ro(path):
    """returns db opened as read only

    Returns
    -------
    Handle to a sqlite3 session
    """
    raise RuntimeError
    db = sqlite3.connect(
        f"file:{path}?mode=ro",
        isolation_level=None,
        detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
        uri=True,
    )
    db.row_factory = sqlite3.Row
    return db


class ReadonlySqliteDataStore(ReadOnlyDataStoreBase):
    # interface to a read only sqlite3 database
    # todo docstring for the class
    store_suffix = "sqlitedb"

    def __init__(self, source, suffix="json", limit=None, verbose=False, md5=True):
        # suffix is hard coded to json
        self._persistent = {k: v for k, v in locals().items() if k != "self"}

        self.suffix = "json"
        if _mem_pattern.search(str(source)):
            self.source = _MEMORY
        else:
            source = pathlib.Path(source).expanduser()
            self.source = (
                source
                if source.suffix[1:] == self.store_suffix  # sliced to remove "."
                else pathlib.Path(f"{source}.{self.store_suffix}")
            )

        self._limit = limit
        self._verbose = verbose
        self._md5 = md5
        self._checksums = {}
        self._db = None
        self._members = []
        self._incomplete = []
        self._open = False

    def __getstate__(self):
        return {**self._persistent}

    def __setstate__(self, state):
        # this will reset connections to read only db's
        obj = self.__class__(**state)
        self.__dict__.update(obj.__dict__)

    def __contains__(self, identifier):
        """whether relative identifier has been stored"""
        if isinstance(identifier, DataStoreMember):
            return identifier.parent is self

        return self.get_member(identifier) is not None

    @property
    def db(self):
        if self._db is None:
            self._db = open_sqlite_db_ro(self.source)
            self._open = True

        return self._db

    def close(self):
        with contextlib.suppress(sqlite3.ProgrammingError):
            self.db.close()

        self._open = False

    def _select_members(self, table_name):
        limit = f"LIMIT {self._limit}" if self._limit else ""
        cmnd = self.db.execute(f"SELECT record_id from {table_name} {limit}")
        return [
            DataStoreMember(r["record_id"], parent=self, id=r["record_id"])
            for r in cmnd.fetchall()
        ]

    def read(self, identifier) -> dict:
        """returns unpickled data from db"""
        return self.open(identifier)

    def open(self, identifier) -> dict:
        if getattr(identifier, "parent", None) is not self:
            member = self.get_member(identifier)
        else:
            member = identifier

        template = "SELECT * FROM {} WHERE record_id = ?"
        for table in (_INCOMPLETE_TABLE, _COMPLETE_TABLE):
            result = self.db.execute(template.format(table), (member,)).fetchone()
            if result:
                break
        else:
            raise ValueError(f"unknown {member}")

        if self._md5:
            self._checksums[member] = get_text_hexdigest(result["data"])

        return _deserialised_compressed(result["data"])

    @property
    def members(self):
        if not self._members:
            self._members = self._select_members(_COMPLETE_TABLE)

        return self._members

    @property
    def incomplete(self):
        """returns database records of type NotCompleted"""
        if not self._incomplete:
            self._incomplete = self._select_members(_INCOMPLETE_TABLE)

        return self._incomplete

    def get_absolute_identifier(self, identifier, from_relative=True):
        """For tinydb, this is the same as the relative identifier"""
        return self.get_relative_identifier(identifier)

    def get_relative_identifier(self, identifier):
        if isinstance(identifier, DataStoreMember) and identifier.parent is self:
            return identifier

        return pathlib.Path(identifier).name

    def get_member(self, identifier):
        """returns DataStoreMember"""
        identifier = self.get_relative_identifier(identifier)
        cmnd_template = "SELECT * FROM {} WHERE record_id = ?"
        for table in (_INCOMPLETE_TABLE, _COMPLETE_TABLE):
            result = self.db.execute(
                cmnd_template.format(table), (identifier,)
            ).fetchone()
            if result:
                return DataStoreMember(
                    result["record_id"], parent=self, id=result["record_id"]
                )

        return None

    @property
    def logs(self):
        """returns all log records"""
        return [
            CompressedValue(r["data"])
            for r in self.db.execute(f"SELECT data FROM {_LOG_TABLE}").fetchall()
        ]

    @property
    def summary_logs(self):
        """returns a table summarising log files"""
        header = ["time", "python version", "who", "command", "composable"]
        rows = []
        for data in self.logs:
            data = data.deserialised.splitlines()
            if not data:
                break
            first = data.pop(0).split("\t")
            row = [first[0]]
            key = None
            mapped = {}
            for line in data:
                line = line.split("\t")[-1].split(" : ", maxsplit=1)
                if len(line) == 1:
                    mapped[key] += line[0]
                    continue

                key = line[0]
                mapped[key] = line[1]

            data = mapped
            row.extend(
                [
                    data["python"],
                    data["user"],
                    data["command_string"],
                    data["composable function"],
                ]
            )
            rows.append(row)
        return Table(
            header=header,
            data=rows,
            title="summary of log files",
        )

    @property
    def _lock_pid(self):
        """returns lock_pid"""
        result = self.db.execute("SELECT lock_pid FROM state").fetchone()
        return result[0] if result else result

    def _record_type(self):
        result = self.db.execute("SELECT record_type FROM state").fetchone()
        return result[0] if result else result

    @property
    def describe(self):
        """returns tables describing content types"""
        if self._lock_pid:
            title = f"Locked db store. Locked to pid={self._lock_pid}, current pid={os.getpid()}."
        else:
            title = "Unlocked db store."

        record_type = self._record_type()
        if record_type:
            title = f"{title} Contains {record_type!r}."
        num_incomplete = len(self.incomplete)
        num_complete = len(self.members)
        num_logs = len(self.logs)
        return Table(
            header=["record type", "number"],
            data=[
                ["completed", num_complete],
                ["incomplete", num_incomplete],
                ["logs", num_logs],
            ],
            title=title,
        )

    @property
    def summary_incomplete(self):
        """returns a table summarising incomplete results"""
        # detect last exception line
        err_pat = re.compile(r"[A-Z][a-z]+[A-Z][a-z]+\:.+")
        types = defaultdict(list)
        indices = "type", "origin"
        header = list(indices) + ["message", "num", "source"]
        rows = []

        if not self.incomplete:
            return Table(header=header, data=rows, title="incomplete records")

        for member in self.incomplete:
            record = {k: v.deserialised for k, v in member.read().items()}
            record = deserialise_not_completed(record)
            key = tuple(getattr(record, k, None) for k in indices)
            match = err_pat.findall(record.message)
            types[key].append([match[-1] if match else record.message, record.source])

        max_string = reprlib.aRepr.maxstring
        reprlib.aRepr.maxstring = 45

        for record in types:
            messages, sources = list(zip(*types[record]))
            messages = reprlib.repr(
                ", ".join(m.splitlines()[-1] for m in set(messages))
            )
            sources = reprlib.repr(", ".join(s.splitlines()[-1] for s in sources))
            row = list(record) + [
                messages,
                len(types[record]),
                sources,
            ]
            rows.append(row)

        reprlib.aRepr.maxstring = max_string  # restoring original val

        return Table(header=header, data=rows, title="incomplete records")


class WriteableSqliteDataStore(ReadonlySqliteDataStore):
    # todo docstring for the class
    store_suffix = "sqlitedb"

    # todo add a id_callback arg, allow users to provide a custom naming function
    # should also be added to the apply_to() method
    def __init__(self, *args, create=True, if_exists="raise", **kwargs):
        super().__init__(*args, **kwargs)
        self._db = None
        self._log_id = None
        self._source_create_delete(if_exists, create)
        self._members = []

    def __getstate__(self):
        raise TypeError(f"{self.__class__.__name__!r} cannot be pickled")

    def __setstate__(self, state):
        raise TypeError(f"{self.__class__.__name__!r} cannot be unpickled")

    def _source_create_delete(self, if_exists, create):
        # todo how well does this match other DataStore's behaviour?
        # add an APPEND if_exists value
        if self.source == _MEMORY:
            return

        if self.source.exists() and if_exists == RAISE:
            raise FileExistsError(f"{self.source}")

        if if_exists == "overwrite":
            self.source.unlink(missing_ok=True)

        if create:
            self.source.parent.mkdir(parents=True, exist_ok=True)
        elif not self.source.absolute().parent.exists():
            raise FileNotFoundError(
                f"'{self.source.parent}' does not exist, set create=True"
            )

    @property
    def db(self):
        if self._db is None:
            self._db = open_sqlite_db_rw(self.source)
            self._open = True
            self._db.execute("pragma journal_mode=wal")  # write-ahead-logging
            with self._db:
                timestamp = datetime.datetime.now()
                self._db.execute(
                    f"INSERT INTO {_LOG_TABLE}(date) VALUES (?)", (timestamp,)
                )
            self._log_id = self._db.execute(
                f"SELECT log_id FROM {_LOG_TABLE} where date = ?", (timestamp,)
            ).fetchone()["log_id"]
            self.lock()  # now record this pid as the lock

        return self._db

    def lock(self, force: bool = True):
        """records of pid as current process pid

        Parameters
        ----------
        force : bool
            ignore an existing lock state
        """
        # insert pid if not already one
        pid = os.getpid()
        with self.db:
            state_id = self._db.execute(
                "SELECT state_id,lock_pid FROM state"
            ).fetchall()

            if len(state_id) > 1:
                raise RuntimeError(
                    "db in inconsistent state, more than one entry in state table"
                )

            if len(state_id) == 1:
                state = state_id[0]
                if state["lock_pid"] != pid and not force:
                    raise RuntimeError(
                        "db locked to other pid, if expected, force unlock"
                    )
                self._db.execute(
                    "UPDATE state SET lock_pid = ? where state_id = ?",
                    (
                        state["state_id"],
                        pid,
                    ),
                )
            else:
                self._db.execute("INSERT INTO state(lock_pid) VALUES (?)", (pid,))

    def unlock(self, force=False):
        """removes a pid lock"""
        if not self._open:
            return

        with self.db:
            row = self.db.execute("SELECT state_id,lock_pid FROM state").fetchall()
            if not row:
                return

            if len(row) > 1:
                raise RuntimeError("inconsistent state")

            row = row[0]
            if not force and row["lock_pid"] != os.getpid():
                return

            self.db.execute(
                "UPDATE state SET lock_pid = NULL WHERE state_id = ?",
                (row["state_id"],),
            )

    def close(self):
        with contextlib.suppress(sqlite3.ProgrammingError):
            self.unlock()
            self.db.commit()
        super().close()

    def _insert_record(self, table_name, identifier, data):
        with self.db:
            self.db.execute(
                f"INSERT INTO {table_name}(record_id,data,log_id) VALUES (?,?,?)",
                (identifier, data, self._log_id),
            )

        m = DataStoreMember(identifier, parent=self, id=identifier)
        self._members.append(m)
        return m

    def write(self, identifier: Optional[Union[str, pathlib.Path]] = None, data=None):
        """writes as binary blob under identifier

        Parameters
        ----------
        identifier : str
            unique value to be written under
        data
            object with to_rich_dict() method, or a dict already
            ready for serialisation

        Notes
        -----
        If is a NotCompleted instance, writes to incomplete table, otherwise
        writes to complete table.

        On write completion, a DataStoreMember is added to self.members.

        md5 checksum is computed and available on this instance
        """
        if identifier is None:
            identifier = get_data_source(data)

        identifier = self.make_identifier(identifier)

        table_name = (
            _INCOMPLETE_TABLE if isinstance(data, NotCompleted) else _COMPLETE_TABLE
        )

        # convert to a dict for serialisation
        with contextlib.suppress(AttributeError):
            data = data.to_rich_dict()

        data = _serialised_compressed(data)
        if self._md5:
            self._checksums[identifier] = get_text_hexdigest(data)

        return self._insert_record(table_name, identifier, data)

    def write_incomplete(self, *args, **kwargs):
        """alias for write"""
        return self.write(*args, **kwargs)

    def add_file(self, path, cleanup=False, **kwargs):
        """alias to add_log()"""
        self.add_log(path, cleanup=cleanup)

    def add_log(self, log, cleanup=False) -> int:
        """
        Parameters
        ----------
        log : str
            text or location of log file to be added to the data store
        cleanup : bool
            delete the original if path

        Returns
        -------
        log id
        """
        try:
            is_file = pathlib.Path(log).exists()
        except OSError:
            is_file = False

        if is_file:
            log = pathlib.Path(log)
            data = log.read_bytes()
        elif type(log) == str:
            data = log.encode("utf8")
        else:
            raise NotImplementedError(f"unexpected type {type(log)} for log")

        data = compress(data)

        with self.db:
            self.db.execute(
                f"UPDATE {_LOG_TABLE} SET data = ? WHERE log_id = ?",
                (
                    data,
                    self._log_id,
                ),
            )

        if is_file and cleanup:
            log.unlink()

        return self._log_id

    # todo this logic, derived from cogent3 data_store, of relative + absolute
    # identifiers, is unnecessary. The only place where an "absolute" identifier
    # makes any sense is for a DirectoryDataStore. I think a refactor is needed:
    # make_identifier
    def make_absolute_identifier(self, data):
        """only a relative identifier makes sense for a db"""
        # todo needs to be deprecated
        return self.make_identifier(data)

    def make_relative_identifier(self, data):
        """alias to make_identifier"""
        # todo needs to be deprecated
        return self.make_identifier(data)

    def make_identifier(self, data):
        """returns identifier for a new member relative to source"""
        identifier = pathlib.Path(get_data_source(data)).name
        # todo is it valuable to add a suffix to each record??
        if not identifier.endswith(self.suffix):
            identifier = f"{identifier}.{self.suffix}"
        return identifier


def _decompressed(data: dict) -> dict:
    for k, v in data.items():
        if isinstance(v, CompressedValue):
            data[k] = v.deserialised
    return data


def load_from_sql():
    deser = get_app("decompress") + get_app("unpickle_it") + get_app("from_primitive")
    return get_app("load_db", deserialiser=deser)


def write_to_sqldb(data_store):
    ser = get_app("to_primitive") + get_app("pickle_it") + get_app("compress")
    return get_app("write_db", data_store=data_store, serialiser=ser)

# this is inheriting from an already defined loader app
class sql_loader(io.load_db):
    def __init__(self, fully_deserialise=True, deserialiser: Callable = None):
        """
        Parameters
        ----------
        fully_deserialise : bool
            completely deserialises objects
        deserialiser : Callable
            function for deserialising objects, defaults to cogent3
            deserialise_object
        """
        from cogent3.util.deserialise import deserialise_object

        super().__init__()

        self._deserialise = fully_deserialise
        self._deserialiser = deserialiser or deserialise_object

    def main(self, identifier: IdentifierType) -> SerialisableType:
        """returns partly deserialised from a Sqlite db"""
        if not self._deserialise:
            return identifier.read()

        data = identifier.read()
        if not hasattr(data, "items"):
            return self._deserialiser(data)

        data = _decompressed(data)
        return self._deserialiser(data)


@define_app(app_type=WRITER)
class sql_writer(io._checkpointable):
    def __init__(self, *args, suffix="json", **kwargs):
        super().__init__(
            *args,
            suffix=suffix,
            writer_class=WriteableSqliteDataStore,
            **kwargs,
        )

    def main(self, data: SerialisableType, identifier=None) -> IdentifierType:
        """

        Parameters
        ----------
        data
            object that has a `to_rich_dict()` method
        identifier : str
            if not provided, taken from data.source or data.info.source

        Returns
        -------
        identifier
        """
        identifier = identifier or get_data_source(data)

        assert identifier is not None

        stored = self.data_store.write(identifier=identifier, data=data)
        if hasattr(data, "info"):
            data.info["stored"] = stored
        else:
            with contextlib.suppress(AttributeError):
                data.stored = stored
        return stored
